{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/research-clone/notebook_tutorials/blob/main/AutoGGUF_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lymmkyinWHp8"
      },
      "outputs": [],
      "source": [
        "# Install llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
        "!pip install -r llama.cpp/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2825nTeWb86"
      },
      "outputs": [],
      "source": [
        "# @title # ‚ö° AutoGGUF v2\n",
        "\n",
        "# @markdown Original from AutoGGUF by [@maximelabonne](https://twitter.com/maximelabonne)\n",
        "\n",
        "\n",
        "# @markdown ‚ù§Ô∏è Created by [qnguyen3](https://twitter.com/stablequan).\n",
        "\n",
        "\n",
        "# @markdown Quantization methods: `q2_k`, `q3_k_l`, `q3_k_m`, `q3_k_s`, `q4_0`, `q4_1`, `q4_k_m`, `q4_k_s`, `q5_0`, `q5_1`, `q5_k_m`, `q5_k_s`, `q6_k`, `q8_0`\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### ‚ö° Quantization parameters\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "MODEL_ID = \"vilm/Quyen-Pro-v0.1\" # @param {type:\"string\"}\n",
        "QUANTIZATION_METHODS = \"q2_k, q3_k_m, q4_0, q4_k_m, q5_0, q5_k_m, q6_k, q8_0\" # @param {type:\"string\"}\n",
        "QUANTIZATION_METHODS = QUANTIZATION_METHODS.replace(\" \", \"\").split(\",\")\n",
        "\n",
        "# @markdown ### ü§ó Hugging Face Hub\n",
        "username = \"vilm\" # @param {type:\"string\"}\n",
        "token = \"HF_TOKEN\" # @param {type:\"string\"}\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
        "snapshot_download(repo_id=MODEL_ID, local_dir=MODEL_NAME,\n",
        "                  local_dir_use_symlinks=False, revision=\"main\")\n",
        "\n",
        "\n",
        "\n",
        "# Convert to fp16\n",
        "fp16 = f\"{MODEL_NAME}/{MODEL_NAME}_fp16.gguf\"\n",
        "!python llama.cpp/convert-hf-to-gguf.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n",
        "\n",
        "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
        "for method in QUANTIZATION_METHODS:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME}_{method.upper()}.gguf\"\n",
        "    !./llama.cpp/quantize {fp16} {qtype} {method}\n",
        "\n",
        "from huggingface_hub import create_repo, HfApi\n",
        "from google.colab import userdata, runtime\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "hf_token = userdata.get(token)\n",
        "api = HfApi()\n",
        "\n",
        "# Upload gguf files\n",
        "api.upload_folder(\n",
        "    folder_path=MODEL_NAME,\n",
        "    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n",
        "    allow_patterns=[\"*.gguf\",\"$.md\"],\n",
        "    token=hf_token\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89CNUdazuN6W"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-wY9BeL8WV4k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}